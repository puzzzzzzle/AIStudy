# 机器学习笔记

## 基础机器学习

### 回归

#### 线性回归

- 基本介绍：通过线性组合特征预测连续值
- 类型：有监督学习
- 函数表达式：$y = \theta_0 + \theta_1x_1 + \cdots + \theta_nx_n$
- 损失函数：均方误差 $J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$

#### 多项式回归

- 基本介绍：用多项式特征扩展的线性回归
- 类型：有监督学习
- 函数表达式：$y = \theta_0 + \theta_1x + \theta_2x^2 + \cdots + \theta_nx^n$
- 损失函数：同线性回归

#### 岭回归（Ridge）

- 基本介绍：带L2正则化的线性回归
- 用途：处理多重共线性问题
- 类型：有监督学习
- 损失函数：$J(\theta) = \text{MSE} + \alpha\sum_{i=1}^n\theta_i^2$

#### Lasso回归

- 基本介绍：带L1正则化的线性回归
- 用途：特征选择与稀疏解
- 类型：有监督学习
- 损失函数：$J(\theta) = \text{MSE} + \alpha\sum_{i=1}^n|\theta_i|$

### 分类

#### 逻辑回归

- 基本介绍：用于二分类的概率模型
- 类型：有监督学习
- 函数表达式：$p = \frac{1}{1+e^{-(\theta^Tx)}}$
- 损失函数：交叉熵损失 $J(\theta) = -\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(p^{(i)}) + (1-y^{(i)})\log(1-p^{(i)})]$

#### 支持向量机（SVM）

- 基本介绍：寻找最大间隔超平面
- 类型：有监督学习
- 函数表达式：$f(x) = \text{sign}(w^T\phi(x) + b)$
- 损失函数：Hinge Loss $\max(0, 1 - y_i(w^Tx_i + b))$

#### 决策树

- 基本介绍：基于特征划分的树形结构
- 类型：有监督学习
- 分裂准则：信息增益/基尼不纯度
- 损失函数：节点不纯度（如基尼系数 $G = 1 - \sum p_i^2$）

#### 随机森林

- 基本介绍：决策树的集成方法
- 类型：有监督学习
- 特点：Bagging+特征随机选择
- 损失函数：同基决策树

#### 梯度提升树（GBDT/XGBoost/LightGBM）

- 基本介绍：加法模型的树集成
- 类型：有监督学习
- 损失函数：自定义可微损失（如对数损失、均方误差）
- 更新方式：梯度下降优化残差

#### 朴素贝叶斯

- 基本介绍：基于贝叶斯定理的概率分类器
- 类型：有监督学习
- 函数表达式：$P(y|x) \propto P(x|y)P(y)$
- 假设：特征条件独立

#### K近邻（KNN）

- 基本介绍：基于样本相似度的惰性学习
- 类型：有监督学习
- 决策规则：多数表决（分类）/平均（回归）
- 距离度量：欧氏/曼哈顿/余弦等

### 聚类

#### K均值聚类（K-Means）

- 基本介绍：基于样本间距离的划分式聚类
- 类型：无监督学习
- 目标函数：$\min \sum_{i=1}^k \sum_{x\in C_i} \|x-\mu_i\|^2$
- 算法步骤：初始化质心→分配样本→更新质心→迭代收敛

#### 均值漂移聚类（Mean Shift）

- 基本介绍：基于密度估计的聚类方法
- 类型：无监督学习
- 核函数：$K(x) = \frac{1}{h^d}k(\|\frac{x-x_i}{h}\|^2)$
- 特点：自动发现聚类数量，适合任意形状分布

#### DBSCAN

- 基本介绍：基于密度的空间聚类
- 类型：无监督学习
- 核心概念：ε邻域、核心点、边界点、噪声点
- 参数：ε（半径）和MinPts（最小点数）

#### 层次聚类

- 基本介绍：通过层次分解/合并构建树状聚类
- 类型：无监督学习
- 方法：凝聚式（自底向上）/分裂式（自顶向下）
- 距离度量：单链接/全链接/平均链接

#### 高斯混合模型（GMM）

- 基本介绍：基于概率分布的软聚类方法
- 类型：无监督学习
- 概率公式：$p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k,\Sigma_k)$
- 优化方法：EM算法估计参数

### 降维

#### 主成分分析（PCA）

- 基本介绍：线性降维，最大化方差投影
- 类型：无监督学习
- 数学原理：协方差矩阵特征分解
- 目标函数：$\max_w \text{Var}(Xw) = w^T\Sigma w$

#### t-SNE

- 基本介绍：基于概率分布的流形学习
- 类型：无监督学习
- 损失函数：KL散度 $C = \sum_i KL(P_i\|Q_i)$
- 特点：保持局部结构，适合可视化

#### LDA

- 基本介绍：线性判别分析（有监督降维）
- 类型：有监督学习
- 目标函数：$\max \frac{w^T S_B w}{w^T S_W w}$
- 用途：分类任务的特征降维

### 模型优化与评估

#### 过拟合与欠拟合

- 过拟合表现：训练误差小，测试误差大
- 欠拟合表现：训练/测试误差都大
- 解决方法：正则化/增加数据（过拟合）；增加模型复杂度（欠拟合）

#### 交叉验证

- 基本方法：k折交叉验证（k=5/10）
- 流程：数据分k份→轮流用1份验证，其余训练→取平均得分
- 目的：评估模型泛化能力

#### 特征工程

- 主要内容：特征选择/构建/提取/缩放
- 常用方法：归一化、离散化、缺失值处理、特征交叉
- 工具：PCA、互信息、卡方检验等

#### 超参数调优（网格搜索/随机搜索）

- 网格搜索：参数网格穷举，计算量大但全面
- 随机搜索：参数空间随机采样，效率更高
- 评估指标：准确率/F1-score等（依任务而定）

#### 评估指标（准确率/精确率/召回率/F1/ROC-AUC）

- 准确率：$\frac{TP+TN}{TP+TN+FP+FN}$
- 精确率：$\frac{TP}{TP+FP}$
- 召回率：$\frac{TP}{TP+FN}$
- F1分数：$2\times\frac{Precision\times Recall}{Precision + Recall}$
- ROC-AUC：ROC曲线下面积，反映排序质量

#### 混淆矩阵

- 结构：
    - 预测正例 预测反例 真实正例 TP FN 真实反例 FP TN
- 衍生指标：灵敏度、特异度、FPR等

## 深度学习

### 基础架构

#### 多层感知器（MLP）

- 基本介绍：全连接的前馈神经网络
- 结构：输入层 + 隐藏层（≥1） + 输出层
- 激活函数：ReLU/Sigmoid/Tanh
- 损失函数：依任务而定（交叉熵/MSE等）

#### 卷积神经网络（CNN）

- 基本介绍：专用于网格数据（图像/视频）的神经网络
- 核心组件：卷积层（滤波器） + 池化层 + 全连接层
- 卷积公式：$y_{i,j} = \sum_{m}\sum_{n} x_{i+m,j+n} \cdot w_{m,n} + b$
- 损失函数：交叉熵（分类）/L2损失（回归）

#### 循环神经网络（RNN/LSTM/GRU）

- 基本介绍：处理序列数据的循环结构
- 时间步公式：$h_t = \sigma(W_{xh}x_t + W_{hh}h_{t-1} + b)$
- 门控机制（LSTM）：
  $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$（遗忘门）
  $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$（输入门）

#### Transformer

- 基本介绍：基于自注意力机制的序列模型
- 核心组件：自注意力 + 前馈网络 + 位置编码
- 注意力公式：$\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$
- 损失函数：交叉熵（带标签平滑）

#### 自编码器（Autoencoder）

- 基本介绍：无监督的特征学习框架
- 结构：编码器（降维） + 解码器（重构）
- 损失函数：重构损失 $L = \|x - \hat{x}\|^2$
- 用途：特征提取/去噪/异常检测

#### 生成对抗网络（GAN）

- 基本介绍：生成器与判别器的对抗训练框架
- 目标函数：$\min_G \max_D V(D,G) = E_{x}[\log D(x)] + E_z[\log(1-D(G(z)))]$
- 训练方式：交替优化生成器和判别器

### 优化技术

#### 批量归一化（BatchNorm）

- 作用：标准化层输入，加速训练
- 公式：$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \cdot \gamma + \beta$
- 效果：改善梯度流动，允许更大学习率

#### Dropout

- 作用：防止过拟合的正则化方法
- 实现：训练时随机丢弃神经元（概率p）
- 推理时：激活值乘以p（近似集成效果）

#### 权重初始化

- 常用方法：
    - Xavier初始化：$W \sim U(-\sqrt{6/(n_{in}+n_{out})}, \sqrt{6/(n_{in}+n_{out})})$
    - He初始化：$W \sim N(0, \sqrt{2/n_{in}})$

#### 优化器（SGD/Adam/RMSProp）

- SGD：$θ_{t+1} = θ_t - \eta \nabla_θ J(θ)$
- Adam：结合动量与自适应学习率
  $m_t = β_1m_{t-1} + (1-β_1)g_t$
  $v_t = β_2v_{t-1} + (1-β_2)g_t^2$

### 进阶应用

#### 注意力机制

- 核心思想：动态分配特征重要性权重
- 缩放点积注意力：$\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$
- 应用：机器翻译/图像描述生成

#### 迁移学习

- 基本方法：冻结预训练模型底层 + 微调顶层
- 常用技术：特征提取/微调/领域自适应
- 优势：小数据场景下的高效学习

#### 预训练模型（BERT/GPT）

- BERT：双向Transformer编码器
    - 预训练任务：MLM + NSP
- GPT：自回归Transformer解码器
    - 预训练任务：语言建模

#### 图神经网络（GNN）

- 基本框架：消息传递 + 节点更新
- 更新公式：$h_v^{(l+1)} = f^{(l)}(h_v^{(l)}, \sum_{u\in N(v)} h_u^{(l)})$
- 应用：社交网络/分子结构/推荐系统

## 强化学习

### 基础理论

#### 马尔科夫决策过程（MDP）

- 基本组成：$〈S, A, P, R, γ〉$
    - S: 状态集合，A: 动作集合
    - P: 状态转移概率 $P(s'|s,a)$
    - R: 奖励函数 $R(s,a,s')$
    - γ: 折扣因子（0≤γ<1）
- 目标：找到最优策略 $π^*: S→A$

#### 贝尔曼方程

- 状态价值函数：$V^π(s) = E_π[\sum_{k=0}^∞ γ^k r_{t+k+1} | s_t = s]$
- 贝尔曼方程：$V^π(s) = \sum_a π(a|s) \sum_{s'} P(s'|s,a)[R(s,a,s') + γV^π(s')]$
- 最优方程：$V^*(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + γV^*(s')]$

#### 探索与利用权衡

- ε-贪婪策略：以1-ε概率选择最优动作，ε概率随机探索
- 上置信界（UCB）：$a_t = \arg\max_a [Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}]$
- 汤普森采样：基于后验分布的概率匹配方法

### 经典算法

#### 动态规划

- 策略迭代：策略评估 + 策略改进
- 值迭代：$V_{k+1}(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + γV_k(s')]$
- 适用条件：已知完整环境模型

#### 蒙特卡洛方法

- 特点：基于完整轨迹的采样
- 首次访问MC更新：$V(s_t) ← V(s_t) + α[G_t - V(s_t)]$
- 探索性出发：保证所有状态-动作对都能被访问

#### 时序差分学习（SARSA/Q-Learning）

- SARSA（同策略）：
  $Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]$
- Q-Learning（异策略）：
  $Q(s,a) ← Q(s,a) + α[r + γ\max_{a'}Q(s',a') - Q(s,a)]$

#### Dyna-Q

- 架构：实际经验 + 模拟经验
- 步骤：
    1. 真实环境交互
    2. 模型学习
    3. 规划阶段（模型生成虚拟经验）

### 深度强化学习

#### DQN（Deep Q-Network）

- 关键技术：
    - 经验回放：打破数据相关性
    - 目标网络：$Q(s,a;θ^-)$
- 损失函数：$L(θ) = E[(r + γ\max_{a'}Q(s',a';θ^-) - Q(s,a;θ))^2]$

#### 策略梯度算法（REINFORCE）

- 梯度公式：$∇_θJ(θ) ∝ E_π[G_t∇_θ\ln π(A_t|S_t,θ)]$
- 特点：高方差，需要基线（baseline）减方差

#### Actor-Critic

- 双网络结构：
    - Actor：策略网络 $π(a|s;θ)$
    - Critic：价值网络 $V(s;w)$
- 更新公式：$δ_t = r_{t+1} + γV(s_{t+1}) - V(s_t)$

#### TRPO/PPO

- TRPO：约束策略更新
  $$\max_θ \mathbb{E}_{s,a \sim π_{θ_{old}}}\left[\frac{π_θ(a|s)}{π_{θ_{old}}(a|s)} A_t\right]$$
  $$\text{s.t. } \mathbb{E}_s\left[D_{KL}(π_{θ_{old}}(\cdot|s) \| π_θ(\cdot|s))\right] \leq δ$$

#### DDPG/SAC

- DDPG（确定性策略）：
    - Actor输出确定性动作
    - 目标网络软更新：$θ' ← τθ + (1-τ)θ'$
- SAC（熵正则化）：
  $J(π) = \sum_t E_{(s_t,a_t)}[r(s_t,a_t) + αH(π(·|s_t))]$

### 扩展方向

#### 模仿学习

- 方法：行为克隆（BC）/逆强化学习（IRL）
- 损失函数：$L(θ) = E_{(s,a)∼D}[-log π_θ(a|s)]$

#### 离线强化学习

- 特点：仅使用静态数据集 $\mathcal{D} = \{(s,a,r,s')\}$
- 挑战：分布偏移（distributional shift）

#### 多智能体强化学习

- 分类：完全合作/竞争/混合
- 挑战：非平稳环境/信用分配

#### 分层强化学习

- 时间抽象：Option框架 $〈I, π, β〉$
    - I: 初始状态集
    - π: 内部策略
    - β: 终止条件

#### 元强化学习

- 目标：学习快速适应新任务的元策略
- MAML公式：$θ' = θ - α∇_θL_{\mathcal{T}}(θ)$