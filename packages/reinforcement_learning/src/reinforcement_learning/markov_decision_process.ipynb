{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Markov Decision Process",
   "id": "202c89e7d1f1f85f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 给定一个马尔可夫过程, 可以从某个状态出发, 根据它的状态转移矩阵生成一个状态序列(episode), 这个步骤也被叫做采样(sampling)",
   "id": "f4d6303889cad884"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 马尔可夫奖励过程\n",
    "马尔可夫奖励过程（Markov Reward Process, MRP）由元组 $\\langle S, P, R, \\gamma \\rangle$ 组成：\n",
    "- $S$ 是有限状态集\n",
    "- $P$ 是状态转移概率矩阵，$P(s'|s) = \\mathbb{P}[S_{t+1}=s'|S_t=s]$\n",
    "- $R$ 是奖励函数，$R(s) = \\mathbb{E}[R_{t+1}|S_t=s]$\n",
    "- $\\gamma$ 是折扣因子，$\\gamma \\in [0,1]$\n",
    "\n",
    "**回报（Return）**定义：\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} $$\n",
    "\n",
    "**状态价值函数（State Value Function）**：\n",
    "$$ V(s) = \\mathbb{E}[G_t|S_t=s] $$\n",
    "\n",
    "**贝尔曼方程**：\n",
    "$$ V(s) = R(s) + \\gamma \\sum_{s' \\in S} P(s'|s)V(s') $$"
   ],
   "id": "102df5ba8fad66be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:09:03.761354Z",
     "start_time": "2025-04-07T12:09:03.756630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# 定义状态转移概率矩阵P\n",
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P = np.array(P)\n",
    "\n",
    "rewards = [-1, -2, -2, 10, 1, 0]  # 定义奖励函数\n",
    "gamma = 0.5  # 定义折扣因子\n",
    "\n",
    "\n",
    "# 给定一条序列,计算从某个索引（起始状态）开始到序列最后（终止状态）得到的回报\n",
    "def compute_return(start_index, chain, gamma):\n",
    "    G = 0\n",
    "    for i in reversed(range(start_index, len(chain))):\n",
    "        G = gamma * G + rewards[chain[i] - 1]\n",
    "    return G\n",
    "\n",
    "\n",
    "# 一个状态序列,s1-s2-s3-s6\n",
    "chain = [1, 2, 3, 6]\n",
    "start_index = 0\n",
    "G = compute_return(start_index, chain, gamma)\n",
    "print(\"根据本序列计算得到回报为：%s。\" % G)"
   ],
   "id": "767a44329030b01c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据本序列计算得到回报为：-2.5。\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:09:06.621584Z",
     "start_time": "2025-04-07T12:09:06.617648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# V = (I - γP)^(-1) * R\n",
    "def compute(P, rewards, gamma, states_num):\n",
    "    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n",
    "    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n",
    "    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n",
    "                   rewards)\n",
    "    return value\n",
    "\n",
    "\n",
    "V = compute(P, rewards, gamma, 6)\n",
    "print(\"MRP中每个状态价值分别为\\n\", V)"
   ],
   "id": "c74d291462a4176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRP中每个状态价值分别为\n",
      " [[-2.01950168]\n",
      " [-2.21451846]\n",
      " [ 1.16142785]\n",
      " [10.53809283]\n",
      " [ 3.58728554]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 马尔可夫决策过程\n",
    "- 智能体的策略（Policy）通常用字母$\\pi$表示\n",
    "- 状态价值函数: $V^\\pi(s) = \\mathbb{E}[G_t|S_t=s, A_t \\sim \\pi]$\n",
    "- 动作价值函数: $Q^\\pi(s,a) = \\mathbb{E}[G_t|S_t=s, A_t=a, A_{t+1} \\sim \\pi]$\n",
    "- 状态价值函数和动作价值函数之间的关系: $V^\\pi(s) = \\sum_a \\pi(a|s)Q^\\pi(s,a)$\n",
    "\n",
    "## 贝尔曼期望方程\n",
    "- 状态价值函数: $V^\\pi(s) = \\sum_a \\pi(a|s) \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a)V^\\pi(s') \\right]$\n",
    "- 动作价值函数: $Q^\\pi(s,a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\sum_a \\pi(a|s')Q^\\pi(s',a)$"
   ],
   "id": "a905b10286d92dd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:39:15.021252Z",
     "start_time": "2025-04-07T12:39:15.016517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n",
    "# 状态转移函数\n",
    "P = {\n",
    "    \"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4,\n",
    "}\n",
    "# 奖励函数\n",
    "R = {\n",
    "    \"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1,\n",
    "}\n",
    "gamma = 0.5  # 折扣因子\n",
    "MDP = (S, A, P, R, gamma)\n",
    "\n",
    "# 策略1,随机策略\n",
    "Pi_1 = {\n",
    "    \"s1-保持s1\": 0.5,\n",
    "    \"s1-前往s2\": 0.5,\n",
    "    \"s2-前往s1\": 0.5,\n",
    "    \"s2-前往s3\": 0.5,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.5,\n",
    "    \"s4-概率前往\": 0.5,\n",
    "}\n",
    "# 策略2\n",
    "Pi_2 = {\n",
    "    \"s1-保持s1\": 0.6,\n",
    "    \"s1-前往s2\": 0.4,\n",
    "    \"s2-前往s1\": 0.3,\n",
    "    \"s2-前往s3\": 0.7,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.1,\n",
    "    \"s4-概率前往\": 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\n",
    "def join(str1, str2):\n",
    "    return str1 + '-' + str2"
   ],
   "id": "802af080ce78e413",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
